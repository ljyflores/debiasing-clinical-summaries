{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1h7ShgonCWoqMdXbXjfAcP69r8X3_PN57",
      "authorship_tag": "ABX9TyOuZaW/wSkWNcPIa2Ott79G"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#BERT Classifier with no distinction between race."
      ],
      "metadata": {
        "id": "3W32V4xUkDoX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if GPU is available."
      ],
      "metadata": {
        "id": "A-AG9r-uibQP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41EXjpn2Cn2r",
        "outputId": "aa071c12-151f-4e7a-e6b1-8e4dab113a62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Dec  4 00:30:13 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P0    25W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import necessary libraries."
      ],
      "metadata": {
        "id": "XLkaHgL0ieY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from scipy.stats import bernoulli\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import percentileofscore\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from datasets import Dataset\n",
        "from transformers import DistilBertTokenizer, DistilBertModel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l51GoaN-Czix",
        "outputId": "deabc1d9-385c-400c-89bf-3f590df5e8ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.7.1-py3-none-any.whl (451 kB)\n",
            "\u001b[K     |████████████████████████████████| 451 kB 32.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2022.11.0)\n",
            "Collecting huggingface-hub<1.0.0,>=0.2.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 73.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.23.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 74.9 MB/s \n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 81.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 69.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: urllib3, xxhash, responses, multiprocess, huggingface-hub, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed datasets-2.7.1 huggingface-hub-0.11.1 multiprocess-0.70.14 responses-0.18.0 urllib3-1.25.11 xxhash-3.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 18.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 47.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "Successfully installed tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess the data."
      ],
      "metadata": {
        "id": "NA4cQgOpinTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell is responsible for preprocessing the dataset.\n",
        "race_keywords = ['AFRICAN-AMERICAN',\n",
        "                'AFRICAN AMERICAN',\n",
        "                'AFRICAN',\n",
        "                'BLACK',\n",
        "                'CREOLE',\n",
        "                'CAUCASIAN',\n",
        "                'WHITE']\n",
        "\n",
        "def remove_keys(s, keywords):\n",
        "    for key in keywords:\n",
        "        if key in s:\n",
        "            s = s.replace(key,'')\n",
        "    return s\n",
        "\n",
        "def clean_string(s):\n",
        "    s = s.replace(',',' ')\n",
        "    s = s.replace('-',' ')\n",
        "    s = s.replace('\\n',' ')\n",
        "    return s\n",
        "\n",
        "# Read data and drop nulls\n",
        "df_adm = pd.read_csv('/content/drive/MyDrive/464ProjectData/ADMISSIONS.csv')\n",
        "df_nte = pd.read_csv('/content/drive/MyDrive/464ProjectData/NOTEEVENTS.csv')\n",
        "df_sev = pd.read_csv('/content/drive/MyDrive/464ProjectData/apsiii-score.csv')\n",
        "\n",
        "df_nte = df_nte.loc[~df_nte.HADM_ID.isnull()].reset_index(drop=True)\n",
        "df_nte['HADM_ID'] = df_nte['HADM_ID'].apply(int)\n",
        "\n",
        "df_adm = df_adm[['SUBJECT_ID','HADM_ID','ETHNICITY','DIAGNOSIS']]\n",
        "\n",
        "# Filter only to nursing notes\n",
        "df_nte = df_nte[['SUBJECT_ID','HADM_ID','CHARTDATE','CATEGORY','TEXT']]\n",
        "df_nte = df_nte.loc[df_nte.CATEGORY.str.contains('Nursing')]\n",
        "df_nte = df_nte.drop_duplicates().reset_index(drop=True)\n",
        "        \n",
        "df_sev = df_sev[['subject_id','hadm_id','apsiii']].drop_duplicates()\n",
        "\n",
        "df = df_adm.merge(df_nte, on=['SUBJECT_ID','HADM_ID'])\n",
        "df = df.merge(df_sev, left_on=['SUBJECT_ID','HADM_ID'], right_on=['subject_id','hadm_id'])\n",
        "\n",
        "# Filter to only white/black patients\n",
        "df = df.loc[df.ETHNICITY.str.contains('WHITE')|df.ETHNICITY.str.contains('BLACK')]\n",
        "df['ETHNICITY'] = df['ETHNICITY'].apply(lambda x: 'WHITE' if 'WHITE' in x else 'BLACK')\n",
        "\n",
        "# Removing race-related keywords from text\n",
        "df['TEXT'] = df['TEXT'].apply(lambda x: x.upper())\n",
        "df['TEXT'] = df['TEXT'].apply(lambda x: remove_keys(x, race_keywords))\n",
        "\n",
        "# Clean punctuations\n",
        "df['TEXT'] = df['TEXT'].apply(clean_string)\n",
        "\n",
        "df = df.drop(['subject_id','hadm_id'], axis=1)\n",
        "df = df.dropna(how='any', axis=0)\n",
        "\n",
        "df.to_csv('/content/drive/MyDrive/464ProjectData/preprocessed.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZUodgW7EYho",
        "outputId": "4c32d34b-536f-46f9-c0ea-899f1c0f27d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py:3326: DtypeWarning: Columns (4,5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set global variables."
      ],
      "metadata": {
        "id": "7TeVdgHwixO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn on GPU computing.\n",
        "device = torch.device(\"cuda\")\n",
        "random.seed(42)\n",
        "\n",
        "# Global configurations.\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS     = 10\n",
        "TAU        = 0.0\n",
        "DIAG_PROP  = 0.8"
      ],
      "metadata": {
        "id": "HnKjH1T4DKLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Samples the treatment disparity between black and white patients.\n",
        "def sample_disparity(actual_treatment, race, tau, diag_prop):\n",
        "    race = race.apply(lambda x: 0 if x=='WHITE' else 1)\n",
        "    bias = race*actual_treatment*tau\n",
        "    prob = actual_treatment.apply(lambda x: x*diag_prop) - bias\n",
        "    return bernoulli.rvs(prob)"
      ],
      "metadata": {
        "id": "WTbrXzMUDKj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the classifier."
      ],
      "metadata": {
        "id": "m3DWFZgIi2CK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The classifier class.\n",
        "class Classifier(nn.Module):\n",
        "    # Note that the output corresponds to treatment prescribed (1) or treatment\n",
        "    # not prescribed (0).\n",
        "    def __init__(self, input_dim, output_dim=1):\n",
        "        super(Classifier, self).__init__()\n",
        "        \n",
        "        self.hidden_layer1 = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "\n",
        "        self.hidden_layer2 = nn.Sequential(\n",
        "            nn.Linear(128, 32),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        self.hidden_layer3 = nn.Sequential(\n",
        "            nn.Linear(32, output_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, labels=None):\n",
        "        output = self.hidden_layer1(x)\n",
        "        output = self.hidden_layer2(output)\n",
        "        output = self.hidden_layer3(output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "R204qXX6DM46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read in the data."
      ],
      "metadata": {
        "id": "X8dOfBMXi_3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/464ProjectData/preprocessed.csv', lineterminator='\\n')\n",
        "df = df.sort_values(by=['SUBJECT_ID','HADM_ID','CHARTDATE'])\\\n",
        "                .groupby(['SUBJECT_ID','HADM_ID'])\\\n",
        "                .head(1)\n",
        "\n",
        "idx_white = random.sample(list(df.loc[df.ETHNICITY=='WHITE'].index), \n",
        "              df.loc[df.ETHNICITY=='BLACK'].shape[0])\n",
        "df = pd.concat([df.loc[idx_white], \n",
        "                df.loc[df.ETHNICITY=='BLACK']]).reset_index(drop=True)\n",
        "\n",
        "df = df[['SUBJECT_ID','ETHNICITY','DIAGNOSIS','TEXT','apsiii']].reset_index(drop=True)\n",
        "df['apsiii_norm'] = list(map(lambda x: percentileofscore(df['apsiii'], x, 'mean')/100, \n",
        "                            df['apsiii']))\n",
        "df['actual_treatment'] = bernoulli.rvs(df['apsiii_norm'])\n",
        "df['given_treatment'] = sample_disparity(df['actual_treatment'], df['ETHNICITY'], TAU, DIAG_PROP)"
      ],
      "metadata": {
        "id": "mvUsizBlDOZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
        "df_train, _ = RandomOverSampler(random_state=42).fit_resample(df_train, df_train['given_treatment'])\n",
        "df_test, _ =  RandomOverSampler(random_state=42).fit_resample(df_test,  df_test['given_treatment'])\n",
        "\n",
        "dataset_train = Dataset.from_pandas(df_train, split='train')\n",
        "dataset_test  = Dataset.from_pandas(df_test,  split='test')\n",
        "\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "dataloader_test  = DataLoader(dataset_test,  batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "AABmPLh_KrE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_stats = df.groupby(['ETHNICITY','actual_treatment']).aggregate({'SUBJECT_ID':'count',\n",
        "                                                        'given_treatment':'sum'})\n",
        "df_stats['prop'] = df_stats['given_treatment']/df_stats['SUBJECT_ID']\n",
        "print(f\"Actual Treatment: {np.mean(df['actual_treatment'])}, Given Treatment: {np.mean(df['given_treatment'])}\")\n",
        "df_stats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "Zhg4yF57KuLd",
        "outputId": "5e181995-88fb-4a2c-ff7c-6d3b13471ff9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual Treatment: 0.49483421432003843, Given Treatment: 0.39440172993753003\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                            SUBJECT_ID  given_treatment      prop\n",
              "ETHNICITY actual_treatment                                       \n",
              "BLACK     0                       2045                0  0.000000\n",
              "          1                       2117             1675  0.791214\n",
              "WHITE     0                       2160                0  0.000000\n",
              "          1                       2002             1608  0.803197"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-87d8e85a-2805-45e8-b61c-1211600d2b42\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>SUBJECT_ID</th>\n",
              "      <th>given_treatment</th>\n",
              "      <th>prop</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ETHNICITY</th>\n",
              "      <th>actual_treatment</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">BLACK</th>\n",
              "      <th>0</th>\n",
              "      <td>2045</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2117</td>\n",
              "      <td>1675</td>\n",
              "      <td>0.791214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">WHITE</th>\n",
              "      <th>0</th>\n",
              "      <td>2160</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2002</td>\n",
              "      <td>1608</td>\n",
              "      <td>0.803197</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-87d8e85a-2805-45e8-b61c-1211600d2b42')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-87d8e85a-2805-45e8-b61c-1211600d2b42 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-87d8e85a-2805-45e8-b61c-1211600d2b42');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(examples, tokenizer):\n",
        "    inputs = examples['TEXT']  \n",
        "    labels = nn.functional.one_hot(examples['given_treatment'], \n",
        "                                   num_classes=2)\n",
        "    tokenized_inputs = tokenizer(inputs,\n",
        "                                 return_tensors='pt',\n",
        "                                 max_length=512,\n",
        "                                 truncation=True,\n",
        "                                 padding=True)\n",
        "    model_inputs = {}\n",
        "    model_inputs['input_ids']      = tokenized_inputs['input_ids']\n",
        "    model_inputs['attention_mask'] = tokenized_inputs['attention_mask']\n",
        "    return model_inputs, labels.float()"
      ],
      "metadata": {
        "id": "7vuJF-VLKvpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use HuggingFace's BERT as the model's encoder.\n",
        "# https://huggingface.co/docs/transformers/model_doc/distilbert#transformers.DistilBertModel\n",
        "model_enc = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\",\n",
        "                                            output_scores=True,\n",
        "                                            output_hidden_states=True,\n",
        "                                            model_max_length=512)\n",
        "# Define the models and optimizer\n",
        "classifier = Classifier(input_dim=512, output_dim=2).to(device)\n",
        "classifier_optimizer = AdamW(classifier.parameters(), \n",
        "                          lr=5e-3)\n",
        "\n",
        "# Define loss\n",
        "criterion = nn.BCELoss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XioYyA-TKxTK",
        "outputId": "baeb2308-67fa-4bb8-9d01-1e183f5710cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_classifier(batch, labels):\n",
        "    classifier.train()\n",
        "    classifier_optimizer.zero_grad()\n",
        "    \n",
        "    x = model_enc(**batch)                                       # Get embeddings from encoder\n",
        "    x = x.last_hidden_state.sum(axis=1).detach()                 # Sum across emb_dim, detach\n",
        "    x = x[:, :512]\n",
        "\n",
        "    pred = classifier(x)                                         # Make prediction\n",
        "    loss = criterion(pred, labels)\n",
        "    loss.backward()\n",
        "    classifier_optimizer.step()\n",
        "    return loss\n",
        "\n",
        "def eval_classifier(batch, labels):\n",
        "    classifier.eval()\n",
        "    with torch.no_grad():\n",
        "        x = model_enc(**batch)                                       # Get embeddings from encoder\n",
        "        x = x.last_hidden_state.sum(axis=1).detach()                 # Sum across emb_dim, detach\n",
        "        x = x[:, :512]\n",
        "\n",
        "        pred = classifier(x)                                         # Make prediction\n",
        "        loss = criterion(pred, labels)\n",
        "        return loss"
      ],
      "metadata": {
        "id": "3jLFDXWMKzBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(ep):\n",
        "    train_loss, test_loss, steps = 0, 0, 0\n",
        "    for batch in dataloader_train:\n",
        "        steps += 1\n",
        "        for i in range(len(batch['ETHNICITY'])):\n",
        "          batch['ETHNICITY'][i] = 'BLACK'\n",
        "        #batch.pop('ETHNICITY', None)\n",
        "        batch, labels           = encode(batch, tokenizer)           # Tokenize and obtain labels\n",
        "        batch['input_ids']      = batch['input_ids'].to(device)      # Send to GPU\n",
        "        batch['attention_mask'] = batch['attention_mask'].to(device)\n",
        "        labels                  = labels.to(device)\n",
        "        train_loss += float(train_classifier(batch, labels).detach().cpu())\n",
        "        # if steps % 500 == 0:\n",
        "        #     torch.save(classifier.state_dict(), f\"results/gan/classifier-{ep}-{steps}-{TAU}-{DIAG_PROP}\")\n",
        "\n",
        "    for batch in dataloader_test:\n",
        "        # batch.pop('ETHNICITY', None)\n",
        "        for i in range(len(batch['ETHNICITY'])):\n",
        "          batch['ETHNICITY'][i] = 'BLACK'\n",
        "        batch, labels           = encode(batch, tokenizer)           # Tokenize and obtain labels\n",
        "        batch['input_ids']      = batch['input_ids'].to(device)      # Send to GPU\n",
        "        batch['attention_mask'] = batch['attention_mask'].to(device)\n",
        "        labels                  = labels.to(device)\n",
        "        test_loss += float(eval_classifier(batch, labels).detach().cpu())\n",
        "    \n",
        "    train_loss /= len(dataloader_train)\n",
        "    test_loss  /= len(dataloader_test)\n",
        "    print(f\"Epoch {ep}: {train_loss}, {test_loss}\")\n",
        "    return train_loss, test_loss"
      ],
      "metadata": {
        "id": "O9mTcocVK6ST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the classifier."
      ],
      "metadata": {
        "id": "MQIgk5K4jtDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_test = np.inf\n",
        "for ep in range(EPOCHS):\n",
        "    train_loss, test_loss = train(ep)\n",
        "    if test_loss < best_test:\n",
        "        best_test = test_loss\n",
        "        torch.save(classifier.state_dict(), f\"/content/drive/MyDrive/464ProjectData/classifier-{TAU}-{DIAG_PROP}-final\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYhm4F7fLAqv",
        "outputId": "9f1f0883-7776-4a59-b6d9-9129b8942913"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: 49.81245192425995, 50.3\n",
            "Epoch 1: 50.00176454626995, 50.15\n",
            "Epoch 2: 50.024070129092976, 50.0\n",
            "Epoch 3: 49.994706383806914, 49.85\n",
            "Epoch 4: 49.994706383806914, 50.0\n",
            "Epoch 5: 50.003529092539914, 49.85\n",
            "Epoch 6: 49.99823546126897, 50.0\n",
            "Epoch 7: 50.00000000753893, 49.85\n",
            "Epoch 8: 49.996470922537945, 50.0\n",
            "Epoch 9: 50.005293623732015, 50.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the classifier."
      ],
      "metadata": {
        "id": "3EVC5vIXj0TS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PNEUMONIA_KEYS = ['PNEUMONIA','PMEUMONIA','PNEUMOMIA',\n",
        "                  'PNEUMONI','PNAUMONIA','PNEMONIA',\n",
        "                  'PNEUMNOIA','PNEUMONIN','PNEUMONNIA']\n",
        "FEVER_KEYS = ['FEVER','FEER']\n",
        "print(df.shape)\n",
        "df_filter = df.loc[df['DIAGNOSIS'].apply(lambda s: any([k in s for k in PNEUMONIA_KEYS]))]\n",
        "print(df_filter.shape)\n",
        "# df_filter = df.loc[df['DIAGNOSIS'].apply(lambda s: any([k in s for k in FEVER_KEYS]))]\n",
        "\n",
        "dataset_filter  = Dataset.from_pandas(df_filter,  split='filter')\n",
        "dataloader_filter = DataLoader(dataset_filter, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf94e8gwLHZS",
        "outputId": "9cb58d68-e656-4963-91fb-297bb4d8aca6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8324, 8)\n",
            "(326, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "race_lst, actual_lst, pred_lst, apsiii_lst = [], [], [], []\n",
        "for idx, batch in enumerate(dataloader_filter):\n",
        "    # print(f\"Predicting: Batch {idx}\")\n",
        "    \n",
        "    # Obtain actual treatment and race features\n",
        "    actual_treatment        = nn.functional.one_hot(batch['actual_treatment'], \n",
        "                                                    num_classes=2).float()\n",
        "    race                    = torch.tensor(list(map(lambda x: 1*(x=='BLACK'), \n",
        "                                                    batch['ETHNICITY'])))\n",
        "    apsiii = batch['apsiii']\n",
        "    \n",
        "    batch, labels           = encode(batch, tokenizer)           # Tokenize and obtain labels\n",
        "    batch['input_ids']      = batch['input_ids'].to(device)      # Send to GPU\n",
        "    batch['attention_mask'] = batch['attention_mask'].to(device)\n",
        "    labels                  = labels.to(device)\n",
        "    \n",
        "    x = model_enc(**batch)                                       # Get embeddings from encoder\n",
        "    x = x.last_hidden_state.sum(axis=1).detach()                 # Sum across emb_dim, detach\n",
        "    x = x[:, :512]\n",
        "    pred = classifier(x).cpu().detach()                          # Make prediction\n",
        "    \n",
        "    race_lst.append(race)\n",
        "    actual_lst.append(actual_treatment)\n",
        "    pred_lst.append(pred)\n",
        "    apsiii_lst.append(apsiii)\n",
        "\n",
        "race_lst   = torch.concat(race_lst)\n",
        "actual_lst = torch.concat(actual_lst)\n",
        "pred_lst   = torch.concat(pred_lst) \n",
        "apsiii_lst = torch.concat(apsiii_lst)\n",
        "\n",
        "print(race_lst.shape, actual_lst.shape, pred_lst.shape, apsiii_lst.shape)\n",
        "\n",
        "idx_0 = torch.where(race_lst==0)[0]\n",
        "idx_1 = torch.where(race_lst==1)[0]\n",
        "\n",
        "print(idx_0, idx_1)\n",
        "\n",
        "acc_1 = float(torch.mean(1.0*(actual_lst[idx_1].argmax(dim=1)==pred_lst[idx_1].argmax(dim=1))))\n",
        "acc_0 = float(torch.mean(1.0*(actual_lst[idx_0].argmax(dim=1)==pred_lst[idx_0].argmax(dim=1))))\n",
        "roc_0 = roc_auc_score(y_true  = actual_lst[idx_0].numpy(),y_score = pred_lst[idx_0].numpy())\n",
        "roc_1 = roc_auc_score(y_true  = actual_lst[idx_1].numpy(),\n",
        "              y_score = pred_lst[idx_1].numpy())\n",
        "print(f\"White: {acc_0} (Acc) {roc_0} ROC-AUC, Black: {acc_1} (Acc) {roc_1} ROC-AUC\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItOUuGtSLIsz",
        "outputId": "20c81e79-cf6c-4905-a4da-d2f995f9feea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([326]) torch.Size([326, 2]) torch.Size([326, 2]) torch.Size([326])\n",
            "tensor([  0,   1,   3,   7,   8,   9,  10,  12,  13,  16,  18,  20,  26,  28,\n",
            "         29,  30,  31,  32,  38,  39,  44,  46,  48,  51,  52,  53,  62,  63,\n",
            "         66,  69,  72,  73,  74,  75,  76,  77,  78,  82,  83,  85,  88,  89,\n",
            "         90,  91,  93,  95,  96,  98, 100, 104, 105, 109, 111, 113, 117, 118,\n",
            "        120, 123, 127, 128, 129, 130, 131, 132, 133, 137, 138, 140, 141, 142,\n",
            "        143, 144, 145, 146, 149, 152, 154, 155, 158, 160, 161, 163, 164, 166,\n",
            "        168, 169, 170, 172, 174, 175, 177, 178, 183, 189, 190, 196, 197, 199,\n",
            "        201, 203, 207, 208, 209, 211, 212, 214, 215, 216, 217, 218, 219, 222,\n",
            "        223, 225, 227, 228, 229, 230, 232, 234, 237, 239, 242, 243, 245, 250,\n",
            "        252, 253, 254, 256, 257, 261, 264, 265, 266, 270, 273, 274, 275, 280,\n",
            "        281, 288, 291, 292, 293, 294, 295, 302, 303, 306, 307, 310, 313, 314,\n",
            "        317, 319, 321, 324, 325]) tensor([  2,   4,   5,   6,  11,  14,  15,  17,  19,  21,  22,  23,  24,  25,\n",
            "         27,  33,  34,  35,  36,  37,  40,  41,  42,  43,  45,  47,  49,  50,\n",
            "         54,  55,  56,  57,  58,  59,  60,  61,  64,  65,  67,  68,  70,  71,\n",
            "         79,  80,  81,  84,  86,  87,  92,  94,  97,  99, 101, 102, 103, 106,\n",
            "        107, 108, 110, 112, 114, 115, 116, 119, 121, 122, 124, 125, 126, 134,\n",
            "        135, 136, 139, 147, 148, 150, 151, 153, 156, 157, 159, 162, 165, 167,\n",
            "        171, 173, 176, 179, 180, 181, 182, 184, 185, 186, 187, 188, 191, 192,\n",
            "        193, 194, 195, 198, 200, 202, 204, 205, 206, 210, 213, 220, 221, 224,\n",
            "        226, 231, 233, 235, 236, 238, 240, 241, 244, 246, 247, 248, 249, 251,\n",
            "        255, 258, 259, 260, 262, 263, 267, 268, 269, 271, 272, 276, 277, 278,\n",
            "        279, 282, 283, 284, 285, 286, 287, 289, 290, 296, 297, 298, 299, 300,\n",
            "        301, 304, 305, 308, 309, 311, 312, 315, 316, 318, 320, 322, 323])\n",
            "White: 0.2830188572406769 (Acc) 0.5 ROC-AUC, Black: 0.371257483959198 (Acc) 0.5 ROC-AUC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_analysis = pd.DataFrame({'race':race_lst.numpy(),\n",
        "              'actual':np.argmax(actual_lst.numpy(), axis=1),\n",
        "              'pred_not_prescribe':pred_lst[:,0].numpy(),\n",
        "              'pred_prescribe':pred_lst[:,1].numpy(),\n",
        "              'apsiii':apsiii_lst.numpy()})\n",
        "df_analysis['prescribe'] = 1*(df_analysis['pred_prescribe'] > df_analysis['pred_not_prescribe'])\n",
        "df_analysis['correct'] = 1*(df_analysis['prescribe']==df_analysis['actual'])\n",
        "\n",
        "for i in range(8):\n",
        "    if i<=5:\n",
        "        temp = df_analysis.loc[(df_analysis.apsiii >= 20*i)&\\\n",
        "                            (df_analysis.apsiii < 20*(i+1))]\\\n",
        "                                .groupby('race')\\\n",
        "                                .aggregate({'correct':'mean', 'prescribe':'mean', 'apsiii':'count'})\n",
        "    else:\n",
        "        temp = df_analysis.loc[(df_analysis.apsiii >= 20*i)]\\\n",
        "                                .groupby('race')\\\n",
        "                                .aggregate({'correct':'mean', 'prescribe':'mean', 'apsiii':'count'})\n",
        "    print(f\"Accuracy (Black): {round(100*temp['correct'][1],3)} ({round(100*temp['prescribe'][1],3)}% prescribed, {temp['apsiii'][1]} patients), \\t Accuracy (White): {round(100*temp['correct'][0],3)} ({round(100*temp['prescribe'][0],3)}% prescribed, {temp['apsiii'][0]} patients)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 695
        },
        "id": "RIG5QwsYLKtt",
        "outputId": "685a3ce3-c7af-4442-e317-0b745ed115fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Black): 66.667 (0.0% prescribed, 3 patients), \t Accuracy (White): 100.0 (0.0% prescribed, 2 patients)\n",
            "Accuracy (Black): 59.259 (0.0% prescribed, 54 patients), \t Accuracy (White): 63.158 (0.0% prescribed, 38 patients)\n",
            "Accuracy (Black): 32.857 (0.0% prescribed, 70 patients), \t Accuracy (White): 22.857 (0.0% prescribed, 70 patients)\n",
            "Accuracy (Black): 8.696 (0.0% prescribed, 23 patients), \t Accuracy (White): 9.677 (0.0% prescribed, 31 patients)\n",
            "Accuracy (Black): 23.077 (0.0% prescribed, 13 patients), \t Accuracy (White): 0.0 (0.0% prescribed, 12 patients)\n",
            "Accuracy (Black): 0.0 (0.0% prescribed, 1 patients), \t Accuracy (White): 0.0 (0.0% prescribed, 5 patients)\n",
            "Accuracy (Black): 0.0 (0.0% prescribed, 3 patients), \t Accuracy (White): 0.0 (0.0% prescribed, 1 patients)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 1",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-bdff9f3b485e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m                                 \u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'race'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                                 \u001b[0;34m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'correct'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'prescribe'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'apsiii'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Accuracy (Black): {round(100*temp['correct'][1],3)} ({round(100*temp['prescribe'][1],3)}% prescribed, {temp['apsiii'][1]} patients), \\t Accuracy (White): {round(100*temp['correct'][0],3)} ({round(100*temp['prescribe'][0],3)}% prescribed, {temp['apsiii'][0]} patients)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_analysis\n"
      ],
      "metadata": {
        "id": "fC-VR1ryLNWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7CfBNUTRLPDO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}