{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Inverse Propensity Weighting"
      ],
      "metadata": {
        "id": "x2RpIjSLiRtU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if GPU is available."
      ],
      "metadata": {
        "id": "A-AG9r-uibQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBDhPY9SbBK4",
        "outputId": "f4b30d7c-695c-4801-f9f1-2ec81d8db8ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Dec  4 00:48:56 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   71C    P0    30W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import necessary libraries."
      ],
      "metadata": {
        "id": "XLkaHgL0ieY6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-gWjTBQ2qqh",
        "outputId": "0c41b197-414b-4f0f-d442-c748d08bfa66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Using cached datasets-2.7.1-py3-none-any.whl (451 kB)\n",
            "Collecting xxhash\n",
            "  Using cached xxhash-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.11.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2022.11.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Collecting multiprocess\n",
            "  Using cached multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Collecting responses<0.19\n",
            "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: urllib3, xxhash, responses, multiprocess, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed datasets-2.7.1 multiprocess-0.70.14 responses-0.18.0 urllib3-1.25.11 xxhash-3.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.25.11)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from scipy.stats import bernoulli\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import percentileofscore\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from datasets import Dataset\n",
        "from transformers import DistilBertTokenizer, DistilBertModel"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess the data."
      ],
      "metadata": {
        "id": "NA4cQgOpinTi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MqdvebB3Zbf",
        "outputId": "e29d5b54-9055-4f85-b10f-fd7024635cdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py:3326: DtypeWarning: Columns (4,5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ],
      "source": [
        "race_keywords = ['AFRICAN-AMERICAN',\n",
        "                'AFRICAN AMERICAN',\n",
        "                'AFRICAN',\n",
        "                'BLACK',\n",
        "                'CREOLE',\n",
        "                'CAUCASIAN',\n",
        "                'WHITE']\n",
        "\n",
        "def remove_keys(s, keywords):\n",
        "    for key in keywords:\n",
        "        if key in s:\n",
        "            s = s.replace(key,'')\n",
        "    return s\n",
        "\n",
        "def clean_string(s):\n",
        "    s = s.replace(',',' ')\n",
        "    s = s.replace('-',' ')\n",
        "    s = s.replace('\\n',' ')\n",
        "    return s\n",
        "\n",
        "# Read data and drop nulls\n",
        "df_adm = pd.read_csv('/content/drive/MyDrive/464ProjectData/ADMISSIONS.csv')\n",
        "df_nte = pd.read_csv('/content/drive/MyDrive/464ProjectData/NOTEEVENTS.csv')\n",
        "df_sev = pd.read_csv('/content/drive/MyDrive/464ProjectData/apsiii-score.csv')\n",
        "\n",
        "df_nte = df_nte.loc[~df_nte.HADM_ID.isnull()].reset_index(drop=True)\n",
        "df_nte['HADM_ID'] = df_nte['HADM_ID'].apply(int)\n",
        "\n",
        "df_adm = df_adm[['SUBJECT_ID','HADM_ID','ETHNICITY','DIAGNOSIS']]\n",
        "\n",
        "# Filter only to nursing notes\n",
        "df_nte = df_nte[['SUBJECT_ID','HADM_ID','CHARTDATE','CATEGORY','TEXT']]\n",
        "df_nte = df_nte.loc[df_nte.CATEGORY.str.contains('Nursing')]\n",
        "df_nte = df_nte.drop_duplicates().reset_index(drop=True)\n",
        "        \n",
        "df_sev = df_sev[['subject_id','hadm_id','apsiii']].drop_duplicates()\n",
        "\n",
        "df = df_adm.merge(df_nte, on=['SUBJECT_ID','HADM_ID'])\n",
        "df = df.merge(df_sev, left_on=['SUBJECT_ID','HADM_ID'], right_on=['subject_id','hadm_id'])\n",
        "\n",
        "# Filter to only white/black patients\n",
        "df = df.loc[df.ETHNICITY.str.contains('WHITE')|df.ETHNICITY.str.contains('BLACK')]\n",
        "df['ETHNICITY'] = df['ETHNICITY'].apply(lambda x: 'WHITE' if 'WHITE' in x else 'BLACK')\n",
        "\n",
        "# Removing race-related keywords from text\n",
        "df['TEXT'] = df['TEXT'].apply(lambda x: x.upper())\n",
        "df['TEXT'] = df['TEXT'].apply(lambda x: remove_keys(x, race_keywords))\n",
        "\n",
        "# Clean punctuations\n",
        "df['TEXT'] = df['TEXT'].apply(clean_string)\n",
        "\n",
        "df = df.drop(['subject_id','hadm_id'], axis=1)\n",
        "df = df.dropna(how='any', axis=0)\n",
        "\n",
        "df.to_csv('/content/drive/MyDrive/464ProjectData/preprocessed_IPW.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set global variables."
      ],
      "metadata": {
        "id": "7TeVdgHwixO9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0Qmn_Mc3ZeH"
      },
      "outputs": [],
      "source": [
        "# Turn on GPU computing.\n",
        "device = torch.device(\"cuda\")\n",
        "random.seed(42)\n",
        "\n",
        "# Global configurations.\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS     = 10\n",
        "TAU        = 0.1\n",
        "DIAG_PROP  = 0.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y964O7w83Zg0"
      },
      "outputs": [],
      "source": [
        "# Samples the treatment disparity between black and white patients.\n",
        "def sample_disparity(actual_treatment, race, tau, diag_prop):\n",
        "    race = race.apply(lambda x: 0 if x=='WHITE' else 1)\n",
        "    bias = race*actual_treatment*tau\n",
        "    prob = actual_treatment.apply(lambda x: x*diag_prop) - bias\n",
        "    return bernoulli.rvs(prob)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the classifier."
      ],
      "metadata": {
        "id": "m3DWFZgIi2CK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLc58LBf3Zjk"
      },
      "outputs": [],
      "source": [
        "# The classifier class.\n",
        "class Classifier(nn.Module):\n",
        "    # Note that the output corresponds to treatment prescribed (1) or treatment\n",
        "    # not prescribed (0).\n",
        "    def __init__(self, input_dim, output_dim=1):\n",
        "        super(Classifier, self).__init__()\n",
        "        \n",
        "        self.hidden_layer1 = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "\n",
        "        self.hidden_layer2 = nn.Sequential(\n",
        "            nn.Linear(128, 32),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        self.hidden_layer3 = nn.Sequential(\n",
        "            nn.Linear(32, output_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, labels=None):\n",
        "        output = self.hidden_layer1(x)\n",
        "        output = self.hidden_layer2(output)\n",
        "        output = self.hidden_layer3(output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read in the data."
      ],
      "metadata": {
        "id": "X8dOfBMXi_3T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPE7zEw03Zme"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/464ProjectData/preprocessed_IPW.csv', lineterminator='\\n')\n",
        "df = df.sort_values(by=['SUBJECT_ID','HADM_ID','CHARTDATE'])\\\n",
        "                .groupby(['SUBJECT_ID','HADM_ID'])\\\n",
        "                .head(1)\n",
        "                \n",
        "idx_white = random.sample(list(df.loc[df.ETHNICITY=='WHITE'].index), \n",
        "              df.loc[df.ETHNICITY=='BLACK'].shape[0])\n",
        "df = pd.concat([df.loc[idx_white], \n",
        "                df.loc[df.ETHNICITY=='BLACK']]).reset_index(drop=True)\n",
        "\n",
        "df = df[['SUBJECT_ID','ETHNICITY','DIAGNOSIS','TEXT','apsiii']].reset_index(drop=True)\n",
        "df['apsiii_norm'] = list(map(lambda x: percentileofscore(df['apsiii'], x, 'mean')/100, \n",
        "                            df['apsiii']))\n",
        "df['actual_treatment'] = bernoulli.rvs(df['apsiii_norm'])\n",
        "df['given_treatment'] = sample_disparity(df['actual_treatment'], df['ETHNICITY'], TAU, DIAG_PROP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdoZafkR3Zoz"
      },
      "outputs": [],
      "source": [
        "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
        "df_train, _ = RandomOverSampler(random_state=42).fit_resample(df_train, df_train['given_treatment'])\n",
        "df_test, _ =  RandomOverSampler(random_state=42).fit_resample(df_test,  df_test['given_treatment'])\n",
        "\n",
        "dataset_train = Dataset.from_pandas(df_train, split='train')\n",
        "dataset_test  = Dataset.from_pandas(df_test,  split='test')\n",
        "\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "dataloader_test  = DataLoader(dataset_test,  batch_size=BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute prop."
      ],
      "metadata": {
        "id": "RIYLp0hmjS1G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9jZi_4c3ZrA",
        "outputId": "2cb8466a-af61-452e-a259-f78a2493de82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual Treatment: 0.5055261893320518, Given Treatment: 0.38322921672272947\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ETHNICITY  actual_treatment\n",
              "BLACK      0                   0.000000\n",
              "           1                   0.719169\n",
              "WHITE      0                   0.000000\n",
              "           1                   0.799315\n",
              "Name: prop, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "df_stats = df.groupby(['ETHNICITY','actual_treatment']).aggregate({'SUBJECT_ID':'count',\n",
        "                                                        'given_treatment':'sum'})\n",
        "df_stats['prop'] = df_stats['given_treatment']/df_stats['SUBJECT_ID']\n",
        "print(f\"Actual Treatment: {np.mean(df['actual_treatment'])}, Given Treatment: {np.mean(df['given_treatment'])}\")\n",
        "df_stats['prop']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "black_patients_ipw = df_stats['prop'][1]\n",
        "white_patients_ipw = df_stats['prop'][3]"
      ],
      "metadata": {
        "id": "NCTUwM7jJ1eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfDFOXRO3Zt2"
      },
      "outputs": [],
      "source": [
        "def encode(examples, tokenizer):\n",
        "    inputs = examples['TEXT']  \n",
        "    labels = nn.functional.one_hot(examples['given_treatment'], \n",
        "                                   num_classes=2)\n",
        "    tokenized_inputs = tokenizer(inputs,\n",
        "                                 return_tensors='pt',\n",
        "                                 max_length=512,\n",
        "                                 truncation=True,\n",
        "                                 padding=True)\n",
        "    model_inputs = {}\n",
        "    model_inputs['input_ids']      = tokenized_inputs['input_ids']\n",
        "    model_inputs['attention_mask'] = tokenized_inputs['attention_mask']\n",
        "    return model_inputs, labels.float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WqOzW3t3ZwB",
        "outputId": "16e806c7-ed28-4813-b502-a3f9ce06d5bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# Use HuggingFace's BERT as the model's encoder.\n",
        "# https://huggingface.co/docs/transformers/model_doc/distilbert#transformers.DistilBertModel\n",
        "model_enc = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\",\n",
        "                                            output_scores=True,\n",
        "                                            output_hidden_states=True,\n",
        "                                            model_max_length=512)\n",
        "# Define the models and optimizer\n",
        "classifier = Classifier(input_dim=512, output_dim=2).to(device)\n",
        "classifier_optimizer = AdamW(classifier.parameters(), \n",
        "                          lr=5e-3)\n",
        "\n",
        "# Define loss\n",
        "criterion = nn.BCELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grZVirFz3Zyv"
      },
      "outputs": [],
      "source": [
        "def train_classifier(batch, labels, weights):\n",
        "    classifier.train()\n",
        "    classifier_optimizer.zero_grad()\n",
        "    \n",
        "    x = model_enc(**batch)                                       # Get embeddings from encoder\n",
        "    x = x.last_hidden_state.sum(axis=1).detach()                 # Sum across emb_dim, detach\n",
        "    x = x[:, :512]\n",
        "\n",
        "    pred = classifier(x)                                         # Make prediction\n",
        "    loss = criterion(pred, labels)\n",
        "    loss = loss * (sum(weights)/len(weights))\n",
        "    loss.backward()\n",
        "    classifier_optimizer.step()\n",
        "    return loss\n",
        "\n",
        "def eval_classifier(batch, labels):\n",
        "    classifier.eval()\n",
        "    with torch.no_grad():\n",
        "        x = model_enc(**batch)                                       # Get embeddings from encoder\n",
        "        x = x.last_hidden_state.sum(axis=1).detach()                 # Sum across emb_dim, detach\n",
        "        x = x[:, :512]\n",
        "\n",
        "        pred = classifier(x)                                         # Make prediction\n",
        "        loss = criterion(pred, labels)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCNT1Dhd3Z1S"
      },
      "outputs": [],
      "source": [
        "def train(ep):\n",
        "    train_loss, test_loss, steps = 0, 0, 0\n",
        "    for batch in dataloader_train:\n",
        "        steps += 1\n",
        "        weights = [float(1/white_patients_ipw) if x == \"WHITE\" else 1/black_patients_ipw for x in batch[\"ETHNICITY\"]]\n",
        "        batch.pop('ETHNICITY', None)\n",
        "        batch, labels           = encode(batch, tokenizer)           # Tokenize and obtain labels\n",
        "        batch['input_ids']      = batch['input_ids'].to(device)      # Send to GPU\n",
        "        batch['attention_mask'] = batch['attention_mask'].to(device)\n",
        "        labels                  = labels.to(device)\n",
        "        train_loss += float(train_classifier(batch, labels, weights).detach().cpu())\n",
        "        # if steps % 500 == 0:\n",
        "        #     torch.save(classifier.state_dict(), f\"results/gan/classifier-{ep}-{steps}-{TAU}-{DIAG_PROP}\")\n",
        "\n",
        "    for batch in dataloader_test:\n",
        "        batch.pop('ETHNICITY', None)\n",
        "        batch, labels           = encode(batch, tokenizer)           # Tokenize and obtain labels\n",
        "        batch['input_ids']      = batch['input_ids'].to(device)      # Send to GPU\n",
        "        batch['attention_mask'] = batch['attention_mask'].to(device)\n",
        "        labels                  = labels.to(device)\n",
        "        test_loss += float(eval_classifier(batch, labels).detach().cpu())\n",
        "    \n",
        "    train_loss /= len(dataloader_train)\n",
        "    test_loss  /= len(dataloader_test)\n",
        "    print(f\"Epoch {ep}: {train_loss}, {test_loss}\")\n",
        "    return train_loss, test_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the classifier."
      ],
      "metadata": {
        "id": "MQIgk5K4jtDP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5Jt1qeE3Z4K",
        "outputId": "7b541013-e1eb-41f8-f9e4-80d68c0b74a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: 65.96137668385225, 50.0\n",
            "Epoch 1: 66.0528232350069, 50.0\n",
            "Epoch 2: 66.04684315101773, 50.0\n",
            "Epoch 3: 66.04684315849752, 50.0\n",
            "Epoch 4: 66.0528232050877, 50.0\n",
            "Epoch 5: 66.05282298817355, 50.0\n",
            "Epoch 6: 66.04684299394197, 50.0\n",
            "Epoch 7: 66.04684315101773, 50.0\n",
            "Epoch 8: 66.06080955804563, 50.0\n",
            "Epoch 9: 66.04086259580126, 50.0\n"
          ]
        }
      ],
      "source": [
        "best_test = np.inf\n",
        "for ep in range(EPOCHS):\n",
        "    train_loss, test_loss = train(ep)\n",
        "    if test_loss < best_test:\n",
        "        best_test = test_loss\n",
        "        torch.save(classifier.state_dict(), f\"/content/drive/MyDrive/464ProjectData/IPW-classifier-{TAU}-{DIAG_PROP}-final\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the classifier."
      ],
      "metadata": {
        "id": "3EVC5vIXj0TS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGlS-SVs3Z68",
        "outputId": "22b6e4cf-876c-4b8e-bcd7-167f2b815974"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8324, 8)\n",
            "(326, 8)\n"
          ]
        }
      ],
      "source": [
        "PNEUMONIA_KEYS = ['PNEUMONIA','PMEUMONIA','PNEUMOMIA',\n",
        "                  'PNEUMONI','PNAUMONIA','PNEMONIA',\n",
        "                  'PNEUMNOIA','PNEUMONIN','PNEUMONNIA']\n",
        "FEVER_KEYS = ['FEVER','FEER']\n",
        "print(df.shape)\n",
        "df_filter = df.loc[df['DIAGNOSIS'].apply(lambda s: any([k in s for k in PNEUMONIA_KEYS]))]\n",
        "print(df_filter.shape)\n",
        "# df_filter = df.loc[df['DIAGNOSIS'].apply(lambda s: any([k in s for k in FEVER_KEYS]))]\n",
        "\n",
        "dataset_filter  = Dataset.from_pandas(df_filter,  split='filter')\n",
        "dataloader_filter = DataLoader(dataset_filter, batch_size=BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcZQq6A-XQWo",
        "outputId": "6344650d-5497-40ec-8b5a-ac79e87ae9df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([326]) torch.Size([326, 2]) torch.Size([326, 2]) torch.Size([326])\n",
            "tensor([  4,   8,   9,  13,  16,  17,  18,  20,  21,  22,  26,  27,  29,  30,\n",
            "         34,  37,  40,  43,  48,  50,  51,  54,  55,  57,  59,  63,  68,  69,\n",
            "         70,  74,  76,  77,  78,  81,  82,  83,  89,  91,  93,  95,  97,  98,\n",
            "        101, 102, 103, 107, 110, 112, 113, 115, 116, 117, 118, 119, 121, 123,\n",
            "        125, 126, 127, 130, 132, 133, 135, 136, 139, 142, 145, 147, 151, 154,\n",
            "        155, 159, 160, 164, 165, 167, 168, 169, 174, 176, 178, 180, 181, 183,\n",
            "        187, 188, 193, 195, 196, 197, 199, 200, 201, 205, 206, 207, 208, 212,\n",
            "        218, 220, 223, 224, 226, 227, 228, 229, 230, 231, 233, 236, 237, 238,\n",
            "        239, 240, 241, 242, 244, 245, 246, 248, 250, 251, 252, 255, 256, 257,\n",
            "        258, 259, 260, 263, 264, 268, 270, 271, 273, 275, 276, 284, 286, 288,\n",
            "        291, 293, 294, 298, 299, 300, 301, 303, 304, 306, 308, 309, 310, 311,\n",
            "        317, 318, 319, 320, 322]) tensor([  0,   1,   2,   3,   5,   6,   7,  10,  11,  12,  14,  15,  19,  23,\n",
            "         24,  25,  28,  31,  32,  33,  35,  36,  38,  39,  41,  42,  44,  45,\n",
            "         46,  47,  49,  52,  53,  56,  58,  60,  61,  62,  64,  65,  66,  67,\n",
            "         71,  72,  73,  75,  79,  80,  84,  85,  86,  87,  88,  90,  92,  94,\n",
            "         96,  99, 100, 104, 105, 106, 108, 109, 111, 114, 120, 122, 124, 128,\n",
            "        129, 131, 134, 137, 138, 140, 141, 143, 144, 146, 148, 149, 150, 152,\n",
            "        153, 156, 157, 158, 161, 162, 163, 166, 170, 171, 172, 173, 175, 177,\n",
            "        179, 182, 184, 185, 186, 189, 190, 191, 192, 194, 198, 202, 203, 204,\n",
            "        209, 210, 211, 213, 214, 215, 216, 217, 219, 221, 222, 225, 232, 234,\n",
            "        235, 243, 247, 249, 253, 254, 261, 262, 265, 266, 267, 269, 272, 274,\n",
            "        277, 278, 279, 280, 281, 282, 283, 285, 287, 289, 290, 292, 295, 296,\n",
            "        297, 302, 305, 307, 312, 313, 314, 315, 316, 321, 323, 324, 325])\n",
            "White: 0.2704402506351471 (Acc) 0.5 ROC-AUC, Black: 0.34730538725852966 (Acc) 0.5 ROC-AUC\n"
          ]
        }
      ],
      "source": [
        "race_lst, actual_lst, pred_lst, apsiii_lst = [], [], [], []\n",
        "for idx, batch in enumerate(dataloader_filter):\n",
        "    # print(f\"Predicting: Batch {idx}\")\n",
        "    \n",
        "    # Obtain actual treatment and race features\n",
        "    actual_treatment        = nn.functional.one_hot(batch['actual_treatment'], \n",
        "                                                    num_classes=2).float()\n",
        "    race                    = torch.tensor(list(map(lambda x: 1*(x=='BLACK'), \n",
        "                                                    batch['ETHNICITY'])))\n",
        "    apsiii = batch['apsiii']\n",
        "    \n",
        "    batch, labels           = encode(batch, tokenizer)           # Tokenize and obtain labels\n",
        "    batch['input_ids']      = batch['input_ids'].to(device)      # Send to GPU\n",
        "    batch['attention_mask'] = batch['attention_mask'].to(device)\n",
        "    labels                  = labels.to(device)\n",
        "    \n",
        "    x = model_enc(**batch)                                       # Get embeddings from encoder\n",
        "    x = x.last_hidden_state.sum(axis=1).detach()                 # Sum across emb_dim, detach\n",
        "    x = x[:, :512]\n",
        "    pred = classifier(x).cpu().detach()                          # Make prediction\n",
        "    \n",
        "    race_lst.append(race)\n",
        "    actual_lst.append(actual_treatment)\n",
        "    pred_lst.append(pred)\n",
        "    apsiii_lst.append(apsiii)\n",
        "\n",
        "race_lst   = torch.concat(race_lst)\n",
        "actual_lst = torch.concat(actual_lst)\n",
        "pred_lst   = torch.concat(pred_lst) \n",
        "apsiii_lst = torch.concat(apsiii_lst)\n",
        "\n",
        "print(race_lst.shape, actual_lst.shape, pred_lst.shape, apsiii_lst.shape)\n",
        "\n",
        "idx_0 = torch.where(race_lst==0)[0]\n",
        "idx_1 = torch.where(race_lst==1)[0]\n",
        "\n",
        "print(idx_0, idx_1)\n",
        "\n",
        "acc_1 = float(torch.mean(1.0*(actual_lst[idx_1].argmax(dim=1)==pred_lst[idx_1].argmax(dim=1))))\n",
        "acc_0 = float(torch.mean(1.0*(actual_lst[idx_0].argmax(dim=1)==pred_lst[idx_0].argmax(dim=1))))\n",
        "roc_0 = roc_auc_score(y_true  = actual_lst[idx_0].numpy(),y_score = pred_lst[idx_0].numpy())\n",
        "roc_1 = roc_auc_score(y_true  = actual_lst[idx_1].numpy(),\n",
        "              y_score = pred_lst[idx_1].numpy())\n",
        "print(f\"White: {acc_0} (Acc) {roc_0} ROC-AUC, Black: {acc_1} (Acc) {roc_1} ROC-AUC\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kf-VbNLFXYqH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 695
        },
        "outputId": "6e8bd7c6-6eb2-40b2-9ce7-6f38ade3d1c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Black): 66.667 (0.0% prescribed, 3 patients), \t Accuracy (White): 100.0 (0.0% prescribed, 2 patients)\n",
            "Accuracy (Black): 62.963 (0.0% prescribed, 54 patients), \t Accuracy (White): 47.368 (0.0% prescribed, 38 patients)\n",
            "Accuracy (Black): 30.0 (0.0% prescribed, 70 patients), \t Accuracy (White): 28.571 (0.0% prescribed, 70 patients)\n",
            "Accuracy (Black): 4.348 (0.0% prescribed, 23 patients), \t Accuracy (White): 9.677 (0.0% prescribed, 31 patients)\n",
            "Accuracy (Black): 0.0 (0.0% prescribed, 13 patients), \t Accuracy (White): 0.0 (0.0% prescribed, 12 patients)\n",
            "Accuracy (Black): 0.0 (0.0% prescribed, 1 patients), \t Accuracy (White): 0.0 (0.0% prescribed, 5 patients)\n",
            "Accuracy (Black): 0.0 (0.0% prescribed, 3 patients), \t Accuracy (White): 0.0 (0.0% prescribed, 1 patients)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 1",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-bdff9f3b485e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m                                 \u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'race'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                                 \u001b[0;34m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'correct'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'prescribe'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'apsiii'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Accuracy (Black): {round(100*temp['correct'][1],3)} ({round(100*temp['prescribe'][1],3)}% prescribed, {temp['apsiii'][1]} patients), \\t Accuracy (White): {round(100*temp['correct'][0],3)} ({round(100*temp['prescribe'][0],3)}% prescribed, {temp['apsiii'][0]} patients)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 1"
          ]
        }
      ],
      "source": [
        "df_analysis = pd.DataFrame({'race':race_lst.numpy(),\n",
        "              'actual':np.argmax(actual_lst.numpy(), axis=1),\n",
        "              'pred_not_prescribe':pred_lst[:,0].numpy(),\n",
        "              'pred_prescribe':pred_lst[:,1].numpy(),\n",
        "              'apsiii':apsiii_lst.numpy()})\n",
        "df_analysis['prescribe'] = 1*(df_analysis['pred_prescribe'] > df_analysis['pred_not_prescribe'])\n",
        "df_analysis['correct'] = 1*(df_analysis['prescribe']==df_analysis['actual'])\n",
        "\n",
        "for i in range(8):\n",
        "    if i<=5:\n",
        "        temp = df_analysis.loc[(df_analysis.apsiii >= 20*i)&\\\n",
        "                            (df_analysis.apsiii < 20*(i+1))]\\\n",
        "                                .groupby('race')\\\n",
        "                                .aggregate({'correct':'mean', 'prescribe':'mean', 'apsiii':'count'})\n",
        "    else:\n",
        "        temp = df_analysis.loc[(df_analysis.apsiii >= 20*i)]\\\n",
        "                                .groupby('race')\\\n",
        "                                .aggregate({'correct':'mean', 'prescribe':'mean', 'apsiii':'count'})\n",
        "    print(f\"Accuracy (Black): {round(100*temp['correct'][1],3)} ({round(100*temp['prescribe'][1],3)}% prescribed, {temp['apsiii'][1]} patients), \\t Accuracy (White): {round(100*temp['correct'][0],3)} ({round(100*temp['prescribe'][0],3)}% prescribed, {temp['apsiii'][0]} patients)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_WCifPOXX9q"
      },
      "outputs": [],
      "source": [
        "df_analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FEtySsb5ltDB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "19u7bA9kpcy_H742nuiosoCZRrLzhcAuF",
      "authorship_tag": "ABX9TyPZA3tkofBSkxBnFBLLdKHk"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}